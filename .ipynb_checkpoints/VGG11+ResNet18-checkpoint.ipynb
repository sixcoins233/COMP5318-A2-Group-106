{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# pip install scikeras==0.10.0\n",
    "# pip install tensorflow-model-optimization==0.7.3\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPU Available: \", gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultility function 1: Training Time Monitoring using tensorflow\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultility function 2: Measure reasoning time\n",
    "def measure_reasoning_time(model, X):\n",
    "    start = time.time()\n",
    "    model.predict(X)\n",
    "    end = time.time()\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultility function 3: Measure the computational complexity of the model (FLOPs)\n",
    "from tensorflow.python.framework import convert_to_constants\n",
    "def get_flops(model):\n",
    "    # Define the input shape for the model\n",
    "    input_shape = (1,) + model.input_shape[1:]\n",
    "    \n",
    "    # Create a concrete function from the model\n",
    "    inputs = tf.TensorSpec(shape=input_shape, dtype=tf.float32)\n",
    "    concrete_function = tf.function(model).get_concrete_function(inputs)\n",
    "    \n",
    "    # Convert variables to constants\n",
    "    frozen_func = convert_to_constants.convert_variables_to_constants_v2(concrete_function)\n",
    "    graph_def = frozen_func.graph.as_graph_def()\n",
    "    \n",
    "    # Import the graph definition\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.compat.v1.import_graph_def(graph_def, name='')\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        \n",
    "        # Run the profiler to calculate FLOPs\n",
    "        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd='scope', options=opts)\n",
    "        \n",
    "    # Return the total FLOPs\n",
    "    return flops.total_float_ops if flops is not None else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the EMNIST data\n",
    "train_dict = pd.read_pickle('emnist_train.pkl')\n",
    "test_dict = pd.read_pickle('emnist_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to np arrays\n",
    "train_images = np.array(train_dict['data'])\n",
    "train_labels = np.array(train_dict['labels'])\n",
    "test_images = np.array(test_dict['data'])\n",
    "test_labels = np.array(test_dict['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a channel dimension\n",
    "train_images = train_images[..., np.newaxis]\n",
    "test_images = test_images[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the images to 32x32\n",
    "train_images = np.array(tf.image.resize(train_images, [32, 32]))\n",
    "test_images = np.array(tf.image.resize(test_images, [32, 32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "print(\"Train Dataset:\", train_dataset)\n",
    "print(\"Test Dataset:\", test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: VGG11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement VGG11 Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG11(input_shape, num_classes=62):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    \n",
    "    # Conv Layer Block 1\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Conv Layer Block 2\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Conv Layer Block 3\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Conv Layer Block 4\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Classifier\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VGG11 baseline model\n",
    "vgg11_model = VGG11(input_shape=(32, 32, 1), num_classes=62)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy()\n",
    "vgg11_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "vgg11_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VGG11 baseline model and measure the training time\n",
    "vgg11_time_callback = TimeHistory()\n",
    "\n",
    "history = vgg11_model.fit(train_dataset.batch(64), validation_data=test_dataset.batch(64), epochs=10, callbacks=[vgg11_time_callback])\n",
    "\n",
    "total_training_time = sum(vgg11_time_callback.times)\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss vs epoch graph\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hidden layers output\n",
    "layer_outputs = [layer.output for layer in vgg11_model.layers if 'max_pooling2d' in layer.name]\n",
    "activation_model = models.Model(inputs=vgg11_model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(test_images[0][np.newaxis, ...])\n",
    "layer_names = [layer.name for layer in vgg11_model.layers if 'max_pooling2d' in layer.name]\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    n_features = layer_activation.shape[-1]\n",
    "    size = layer_activation.shape[1]      \n",
    "    plt.figure(figsize=(12,12))\n",
    "    for i in range(n_features):\n",
    "        # Display only the first 64 features\n",
    "        if i >= 64:\n",
    "            break\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow(layer_activation[0, :, :, i], cmap='viridis')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"Layer: {layer_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the model\n",
    "def create_model_vgg11(learning_rate, optimizer_name):\n",
    "    optimizer_dict = {\n",
    "    'adam': keras.optimizers.Adam,\n",
    "    'adagrad': keras.optimizers.Adagrad,\n",
    "    'rmsprop': keras.optimizers.RMSprop\n",
    "    }\n",
    "    if optimizer_name in optimizer_dict:\n",
    "        optimizer = optimizer_dict[optimizer_name](learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "    model = VGG11(input_shape=(32, 32, 1), num_classes=62)\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy()\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "param_grid = {\n",
    "    'model__learning_rate': [1e-5, 5e-5, 1e-4],\n",
    "    'model__optimizer_name': ['adam', 'adagrad','rmsprop']\n",
    "    \n",
    "    # Debug parameters\n",
    "    # 'model__learning_rate': [1e-4],\n",
    "    # 'model__optimizer_name': ['adam']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model using KerasClassifier\n",
    "# Use scikeras.wrappers.KerasClassifier to wrap the model.\n",
    "keras_model = KerasClassifier(model=create_model_vgg11, epochs=10)\n",
    "\n",
    "# Perform the grid search\n",
    "skf = StratifiedKFold(n_splits=1, shuffle=True)\n",
    "grid = GridSearchCV(estimator=keras_model, param_grid=param_grid, cv=skf)\n",
    "grid_result = grid.fit(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "# Mean measures the best-performing hyperparameter combination.\n",
    "# Standard deviation measures the stability of the model's performance.\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, std, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the best hyperparameters\n",
    "best_lr_vgg11 = grid_result.best_params_['model__learning_rate']\n",
    "best_opt_vgg11 = grid_result.best_params_['model__optimizer_name']\n",
    "best_model_vgg11 = create_model_vgg11(best_lr_vgg11, best_opt_vgg11)\n",
    "history = best_model_vgg11.fit(train_dataset.batch(64), validation_data=test_dataset.batch(64), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model_vgg11.predict(test_dataset.batch(64))\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "vgg11_accuracy = accuracy_score(y_true, y_pred)\n",
    "vgg11_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "vgg11_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "vgg11_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "vgg11_top5_accuracy = top_k_accuracy_score(y_true, best_model_vgg11.predict(test_dataset.batch(64)), k=5)\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement ResNet18 Network Structure\n",
    "K. He, X. Zhang, S. Ren and J. Sun, \"Deep Residual Learning for Image Recognition,\" 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 770-778, doi: 10.1109/CVPR.2016.90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, kernel_size=3, downsample=False):\n",
    "    shortcut = x\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=kernel_size, strides=(2 if downsample else 1), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(filters, kernel_size=kernel_size, strides=1, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Add downsample layer\n",
    "    if downsample:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=1, strides=2, padding='same')(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    # Add shortcut connection\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def ResNet18(input_shape, num_classes=62):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Conv Layer Block 1\n",
    "    x = layers.Conv2D(64, (7, 7), strides=2, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=2, padding='same')(x)\n",
    "\n",
    "    # Residual Block 1\n",
    "    x = residual_block(x, filters=64)\n",
    "    x = residual_block(x, filters=64)\n",
    "\n",
    "    # Residual Block 2\n",
    "    x = residual_block(x, filters=128, downsample=True)\n",
    "    x = residual_block(x, filters=128)\n",
    "\n",
    "    # Residual Block 3\n",
    "    x = residual_block(x, filters=256, downsample=True)\n",
    "    x = residual_block(x, filters=256)\n",
    "\n",
    "    # Residual Block 4\n",
    "    # x = residual_block(x, filters=512, downsample=True)\n",
    "    # x = residual_block(x, filters=512)\n",
    "\n",
    "    # Classifier\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ResNet18 baseline model\n",
    "resnet18_model = ResNet18(input_shape=(32, 32, 1), num_classes=62)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy()\n",
    "resnet18_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "resnet18_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the baseline model\n",
    "resnet18_time_callback = TimeHistory()\n",
    "\n",
    "history = resnet18_model.fit(train_dataset.batch(64), validation_data=test_dataset.batch(64), epochs=10, callbacks=[resnet18_time_callback])\n",
    "\n",
    "total_training_time = sum(resnet18_time_callback.times)\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss vs epoch graph\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hidden layers output\n",
    "layer_outputs = [layer.output for layer in resnet18_model.layers if 'conv2d_117' in layer.name or 'conv2d_122' in layer.name or 'conv2d_127' in layer.name]\n",
    "activation_model = models.Model(inputs=resnet18_model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(test_images[0][np.newaxis, ...])\n",
    "layer_names = [layer.name for layer in resnet18_model.layers if 'conv2d_117' in layer.name or 'conv2d_122' in layer.name or 'conv2d_127' in layer.name]\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    n_features = layer_activation.shape[-1]\n",
    "    size = layer_activation.shape[1]      \n",
    "    plt.figure(figsize=(16,16))\n",
    "    for i in range(n_features):\n",
    "        # Display only the first 64 features\n",
    "        if i >= 64:\n",
    "            break\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow(layer_activation[0, :, :, i], cmap='viridis')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"Layer: {layer_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the model\n",
    "def create_model_resnet18(learning_rate, optimizer_name):\n",
    "    optimizer_dict = {\n",
    "    'adam': keras.optimizers.Adam,\n",
    "    'adagrad': keras.optimizers.Adagrad,\n",
    "    'rmsprop': keras.optimizers.RMSprop\n",
    "    }\n",
    "    if optimizer_name in optimizer_dict:\n",
    "        optimizer = optimizer_dict[optimizer_name](learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "    model = ResNet18(input_shape=(32, 32, 1), num_classes=62)\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy()\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "param_grid = {\n",
    "    'model__learning_rate': [5e-5, 1e-4, 5e-4],\n",
    "    'model__optimizer_name': ['adam', 'adagrad','rmsprop']\n",
    "\n",
    "    # Debug parameters\n",
    "    # 'model__learning_rate': [1e-5],\n",
    "    # 'model__optimizer_name': ['adam']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model using KerasClassifier\n",
    "# Use scikeras.wrappers.KerasClassifier to wrap the model.\n",
    "keras_model = KerasClassifier(model=create_model_resnet18, epochs=10)\n",
    "\n",
    "# Perform the grid search\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "grid = GridSearchCV(estimator=keras_model, param_grid=param_grid, cv=skf)\n",
    "grid_result = grid.fit(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, std, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the best hyperparameters\n",
    "best_lr_resnet18 = grid_result.best_params_['model__learning_rate']\n",
    "best_opt_resnet18 = grid_result.best_params_['model__optimizer_name']\n",
    "best_model_resnet18 = create_model_resnet18(best_lr_resnet18, best_opt_resnet18)\n",
    "history = best_model_resnet18.fit(train_dataset.batch(64), validation_data=test_dataset.batch(64), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model_resnet18.predict(test_dataset.batch(64))\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "resnet18_accuracy = accuracy_score(y_true, y_pred)\n",
    "resnet18_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "resnet18_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "resnet18_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "resnet18_top5_accuracy = top_k_accuracy_score(y_true, best_model_resnet18.predict(test_dataset.batch(64)), k=5)\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'VGG11': best_model_vgg11,\n",
    "    'ResNet18': best_model_resnet18,\n",
    "    # 'EfficientNetB0': best_model_efficientnetb0,\n",
    "}\n",
    "acc = {\n",
    "    'VGG11': vgg11_accuracy,\n",
    "    'ResNet18': resnet18_accuracy,\n",
    "    # 'EfficientNetB0': efficientnetb0_accuracy,\n",
    "}\n",
    "training_time = {\n",
    "    'VGG11': sum(vgg11_time_callback.times),\n",
    "    'ResNet18': sum(resnet18_time_callback.times),\n",
    "    # 'EfficientNetB0': sum(efficientnetb0_time_callback.times),\n",
    "}\n",
    "reasoning_time = {\n",
    "    'VGG11': measure_reasoning_time(best_model_vgg11, test_images),\n",
    "    'ResNet18': measure_reasoning_time(best_model_resnet18, test_images),\n",
    "    # 'EfficientNetB0': measure_reasoning_time(best_model_efficientnetb0, test_images),\n",
    "}\n",
    "flops = {\n",
    "    'VGG11': get_flops(best_model_vgg11),\n",
    "    'ResNet18': get_flops(best_model_resnet18),\n",
    "    # 'EfficientNetB0': get_flops(best_model_efficientnetb0),\n",
    "}\n",
    "for name, model in models.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {acc[name]:.4f}\")\n",
    "    print(f\"Training Time: {training_time[name]:.2f} seconds\")\n",
    "    print(f\"Reasoning Time: {reasoning_time[name]:.4f} seconds\")\n",
    "    print(f\"FLOPs: {flops[name]:.2f}\")\n",
    "    print(\"\\n\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'Top-5 Accuracy']\n",
    "vgg11_metrics = [vgg11_accuracy, vgg11_precision, vgg11_recall, vgg11_f1, vgg11_top5_accuracy]\n",
    "resnet18_metrics = [resnet18_accuracy, resnet18_precision, resnet18_recall, resnet18_f1, resnet18_top5_accuracy]\n",
    "# efficientnetb0_metrics = [efficientnetb0_accuracy, efficientnetb0_precision, efficientnetb0_recall, efficientnetb0_f1, efficientnetb0_top5_accuracy]\n",
    "\n",
    "# Plot the performance metrics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//3, i%3]\n",
    "    ax.bar(['VGG11', 'ResNet18'], [vgg11_metrics[i], resnet18_metrics[i]])\n",
    "    # ax.bar(['VGG11', 'ResNet18', 'EfficientNetB0'], [vgg11_metrics[i], resnet18_metrics[i], efficientnetb0_metrics[i]])\n",
    "    ax.set_title(metric + ' Comparison')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xlabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training time metric\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.bar(['VGG11', 'ResNet18'], [training_time['VGG11'], training_time['ResNet18']])\n",
    "# ax.bar(['VGG11', 'ResNet18', 'EfficientNetB0'], [training_time['VGG11'], training_time['ResNet18'], training_time['EfficientNetB0']])\n",
    "ax.set_title('Training Time Comparison')\n",
    "ax.set_ylabel('Training Time (s)')\n",
    "ax.set_xlabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reasoning time metric\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.bar(['VGG11', 'ResNet18'], [reasoning_time['VGG11'], reasoning_time['ResNet18']])\n",
    "# ax.bar(['VGG11', 'ResNet18', 'EfficientNetB0'], [reasoning_time['VGG11'], reasoning_time['ResNet18'], reasoning_time['EfficientNetB0']])\n",
    "ax.set_title('Reasoning Time Comparison')\n",
    "ax.set_ylabel('Reasoning Time (s)')\n",
    "ax.set_xlabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the FLOPs metric\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.bar(['VGG11', 'ResNet18'], [flops['VGG11'], flops['ResNet18']])\n",
    "# ax.bar(['VGG11', 'ResNet18', 'EfficientNetB0'], [flops['VGG11'], flops['ResNet18'], flops['EfficientNetB0']])\n",
    "ax.set_title('FLOPs Comparison')\n",
    "ax.set_ylabel('FLOPs')\n",
    "ax.set_xlabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
